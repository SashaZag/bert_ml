{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1718f5a7-757c-4ba4-9336-ccca8690fb21",
      "metadata": {
        "id": "1718f5a7-757c-4ba4-9336-ccca8690fb21"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn import preprocessing\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import transformers as transform\n",
        "import torch\n",
        "from torch.nn.functional import softmax, normalize\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/MyDrive')"
      ],
      "metadata": {
        "id": "aJlmoB8ycBf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99cd6aad-f8f8-4c4c-aff4-14e97526a559"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /MyDrive; to attempt to forcibly remount, call drive.mount(\"/MyDrive\", force_remount=True).\n"
          ]
        }
      ],
      "id": "aJlmoB8ycBf4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2f194ce-bbb0-4038-8305-c86578cd3db8",
      "metadata": {
        "id": "d2f194ce-bbb0-4038-8305-c86578cd3db8"
      },
      "outputs": [],
      "source": [
        "class Bert_Regression:\n",
        "\n",
        "    model_class = transform.DistilBertForSequenceClassification\n",
        "\n",
        "    tokenizer_class = transform.DistilBertTokenizerFast\n",
        "\n",
        "    pretrained_weights = 'distilbert-base-uncased'\n",
        "    tokenizer = 0\n",
        "    model = 0\n",
        "\n",
        "    def initilize_tokenizer(self):\n",
        "        self.tokenizer = self.tokenizer_class.from_pretrained(self.pretrained_weights)\n",
        "\n",
        "        return self.tokenizer\n",
        "\n",
        "    def initilize_model(self):\n",
        "        self.model = self.model_class.from_pretrained(self.pretrained_weights, num_labels=10, max_position_embeddings=512)\n",
        "\n",
        "\n",
        "        return self.model\n",
        "\n",
        "    def clear_model_and_tokenizer(self):\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "\n",
        "    def get_reviewer_data(self, reviewer_names):\n",
        "\n",
        "        reviewer_ids = pd.DataFrame(columns=['ids'])\n",
        "        reviewer_ratings = pd.DataFrame(columns=['ratings'])\n",
        "\n",
        "        reviewer_file_ids = ['../MyDrive/MyDrive/scaledata/'+reviewer_name+'/id.'+reviewer_name for reviewer_name in reviewer_names]\n",
        "        reviewer_file_ratings = ['../MyDrive/MyDrive/scaledata/'+reviewer_name+'/rating.'+reviewer_name for reviewer_name in reviewer_names]\n",
        "        reviews_folder = ['../MyDrive/MyDrive/scale_whole_review/'+reviewer_name+'/txt.parag' for reviewer_name in reviewer_names]\n",
        "\n",
        "        count = 0\n",
        "\n",
        "        for reviewer_id in reviewer_file_ids:\n",
        "            print(reviewer_file_ids)\n",
        "            id_rating = pd.read_csv(reviewer_id, names = ['ids'])\n",
        "            id_rating['full_review'] = ''\n",
        "\n",
        "            for id in id_rating['ids']:\n",
        "                path = reviews_folder[count]+'/'+str(id)+'.txt'\n",
        "                f = open(path, 'r', encoding='cp1252')\n",
        "                text = f.read()\n",
        "                text = re.sub(r'[^\\w]', ' ', text)\n",
        "                text = re.sub(\"\\d+\", \"\", text)\n",
        "                text = text.lower()\n",
        "                id_rating.loc[id_rating['ids'] == id, 'full_review'] = text\n",
        "                # id_rating.loc[id == dennis_fulls, 'full_review'] = text\n",
        "                # print(id_rating)\n",
        "                f.close()\n",
        "                # print(path)\n",
        "\n",
        "            reviewer_ids = pd.concat([reviewer_ids, id_rating], axis=0)\n",
        "            count = count + 1\n",
        "\n",
        "        for reviewer_rating in reviewer_file_ratings:\n",
        "            # print(reviewer_id)\n",
        "            id_rating = pd.read_csv(reviewer_rating, names = ['ratings'])#.round()\n",
        "\n",
        "            reviewer_ratings = pd.concat([reviewer_ratings, id_rating], axis=0)\n",
        "            # print(reviewer_ratings)\n",
        "\n",
        "\n",
        "        reviewer_ids.reset_index(drop=True, inplace=True)\n",
        "        reviewer_ratings.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        reviewer_data = pd.concat([reviewer_ids, reviewer_ratings], axis=1)\n",
        "\n",
        "\n",
        "        return reviewer_data\n",
        "\n",
        "    def align_data(self, tokenized):\n",
        "        max_length = max([len(i) for i in tokenized])\n",
        "\n",
        "        features = pd.DataFrame(columns=range(max_length))\n",
        "\n",
        "        for i in range(len(tokenized)):\n",
        "            # features = features.append(pd.DataFrame(dennis_full['tokenized_sentences'][i]).transpose())\n",
        "            features = pd.concat([features, pd.DataFrame(tokenized[i]).transpose()])\n",
        "            # features = features.shift(-1)\n",
        "            # features.iloc[-1, :] = features.append(pd.DataFrame(dennis_full['tokenized_sentences'][i]).transpose())\n",
        "        features.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        return features\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenData(Dataset):\n",
        "    def __init__(self, train = False):\n",
        "        if train:\n",
        "            self.text_data = reviewers_data['full_review']\n",
        "            self.tokens = train_tokens\n",
        "            self.labels = list(reviewers_data['ratings'] * 10)\n",
        "        else:\n",
        "            self.text_data = reviewers_test_data['full_review']\n",
        "            self.tokens = test_tokens\n",
        "            self.labels = list(reviewers_test_data['ratings'] * 10)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = {}\n",
        "        for k, v in self.tokens.items():\n",
        "            sample[k] = torch.tensor(v[idx])\n",
        "        sample['labels'] = torch.tensor(self.labels[idx])\n",
        "        return sample"
      ],
      "metadata": {
        "id": "X7Z62M8eTXo3"
      },
      "id": "X7Z62M8eTXo3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01d360f7-4d13-445a-a5a6-a34163b47dc7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01d360f7-4d13-445a-a5a6-a34163b47dc7",
        "outputId": "898740d4-20d3-474c-f78c-7651e9f43e4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['../MyDrive/MyDrive/scaledata/Dennis+Schwartz/id.Dennis+Schwartz']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "#Training dataset\n",
        "x = Bert_Regression()\n",
        "reviewers_data = x.get_reviewer_data(['Dennis+Schwartz'])\n",
        "#James+Berardinelli Dennis+Schwartz Scott+Renshaw\n",
        "tokenizer = x.initilize_tokenizer()\n",
        "\n",
        "train_tokens = tokenizer(list(reviewers_data['full_review']), padding='max_length', truncation=False, max_length=3210)\n",
        "train_tokens['labels'] = reviewers_data['ratings'] * 10"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test dataset\n",
        "reviewers_test_data = x.get_reviewer_data(['Scott+Renshaw'])\n",
        "#James+Berardinelli Dennis+Schwartz Scott+Renshaw\n",
        "\n",
        "test_tokens = tokenizer(list(reviewers_test_data['full_review']), padding='max_length', truncation=False, max_length=3210)\n",
        "test_tokens['labels'] = reviewers_test_data['ratings'] * 10"
      ],
      "metadata": {
        "id": "nkmixAXn42za",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ad4c980-c44d-44de-cf65-2db97962cb9a"
      },
      "id": "nkmixAXn42za",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['../MyDrive/MyDrive/scaledata/Scott+Renshaw/id.Scott+Renshaw']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "807bba9a-1871-4082-8cf4-eb1487ce7fa1",
      "metadata": {
        "id": "807bba9a-1871-4082-8cf4-eb1487ce7fa1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e2331b4-cd50-4a83-cb97-3640cbeeacd0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_mask', 'labels'])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "train_tokens.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13447756-e5d7-44f0-9ede-40597faad64a",
      "metadata": {
        "id": "13447756-e5d7-44f0-9ede-40597faad64a"
      },
      "outputs": [],
      "source": [
        "#Train data\n",
        "batch_size = 1\n",
        "train_data = TokenData(train = True)\n",
        "train_dataset = DataLoader(train_data, shuffle=True, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test data\n",
        "batch_size = 1\n",
        "test_data = TokenData(train = False)\n",
        "test_dataset = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "YvV3PRTY5xA6"
      },
      "id": "YvV3PRTY5xA6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6600d4e-3900-4bb4-ba0b-cc451cc314fb",
      "metadata": {
        "id": "b6600d4e-3900-4bb4-ba0b-cc451cc314fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "669ceca5-f6d0-4715-b4bb-702849a8d9ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "bert_model = x.initilize_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5a660b2-d257-4e45-8fce-aed4cadd0b4b",
      "metadata": {
        "id": "b5a660b2-d257-4e45-8fce-aed4cadd0b4b"
      },
      "outputs": [],
      "source": [
        "# optim func\n",
        "optimizer = torch.optim.AdamW(bert_model.parameters(), lr=1e-3)\n",
        "# loss func\n",
        "loss_fn = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model.config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b47trOW8YCKU",
        "outputId": "b11ebfdb-ab6b-4e8e-d2f5-5ebc309ff11b"
      },
      "id": "b47trOW8YCKU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertConfig {\n",
              "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
              "  \"activation\": \"gelu\",\n",
              "  \"architectures\": [\n",
              "    \"DistilBertForMaskedLM\"\n",
              "  ],\n",
              "  \"attention_dropout\": 0.1,\n",
              "  \"dim\": 768,\n",
              "  \"dropout\": 0.1,\n",
              "  \"hidden_dim\": 3072,\n",
              "  \"id2label\": {\n",
              "    \"0\": \"LABEL_0\",\n",
              "    \"1\": \"LABEL_1\",\n",
              "    \"2\": \"LABEL_2\",\n",
              "    \"3\": \"LABEL_3\",\n",
              "    \"4\": \"LABEL_4\",\n",
              "    \"5\": \"LABEL_5\",\n",
              "    \"6\": \"LABEL_6\",\n",
              "    \"7\": \"LABEL_7\",\n",
              "    \"8\": \"LABEL_8\",\n",
              "    \"9\": \"LABEL_9\"\n",
              "  },\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"label2id\": {\n",
              "    \"LABEL_0\": 0,\n",
              "    \"LABEL_1\": 1,\n",
              "    \"LABEL_2\": 2,\n",
              "    \"LABEL_3\": 3,\n",
              "    \"LABEL_4\": 4,\n",
              "    \"LABEL_5\": 5,\n",
              "    \"LABEL_6\": 6,\n",
              "    \"LABEL_7\": 7,\n",
              "    \"LABEL_8\": 8,\n",
              "    \"LABEL_9\": 9\n",
              "  },\n",
              "  \"max_position_embeddings\": 512,\n",
              "  \"model_type\": \"distilbert\",\n",
              "  \"n_heads\": 12,\n",
              "  \"n_layers\": 6,\n",
              "  \"pad_token_id\": 0,\n",
              "  \"qa_dropout\": 0.1,\n",
              "  \"seq_classif_dropout\": 0.2,\n",
              "  \"sinusoidal_pos_embds\": false,\n",
              "  \"tie_weights_\": true,\n",
              "  \"transformers_version\": \"4.41.2\",\n",
              "  \"vocab_size\": 30522\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model.to(\"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a373IohqxajG",
        "outputId": "f0142c10-8419-414e-9629-f44e20c84540"
      },
      "id": "a373IohqxajG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertForSequenceClassification(\n",
              "  (distilbert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_tokens['attention_mask'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iu6UYwJdhWbf",
        "outputId": "7ff46564-0784-4f7b-b6d6-e041500a3a90"
      },
      "id": "Iu6UYwJdhWbf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1027"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec7b1efa-5581-43a7-bfd3-a17647cbc7a0",
      "metadata": {
        "id": "ec7b1efa-5581-43a7-bfd3-a17647cbc7a0"
      },
      "outputs": [],
      "source": [
        "epochs = 3\n",
        "train_last_loss = 0\n",
        "multiplier = 0\n",
        "logits = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(\"Epoch: \",(epoch + 1))\n",
        "    predicted = 0\n",
        "\n",
        "    bert_model.train()\n",
        "\n",
        "    iter_number = 0\n",
        "\n",
        "    for i, values in enumerate(train_dataset):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for multiplier in range(6):\n",
        "\n",
        "            val1 = torch.tensor(values['input_ids'][0][multiplier*512:(multiplier+1)*512])\n",
        "            val1 = val1.unsqueeze(dim=0)\n",
        "            val2 = torch.tensor(values['attention_mask'][0][multiplier*512:(multiplier+1)*512])\n",
        "            val2 = val2.unsqueeze(dim=0)\n",
        "\n",
        "            outputs = bert_model(val1, val2)\n",
        "            logits = logits + outputs.logits\n",
        "\n",
        "        logits = logits/6\n",
        "        loss = loss_fn(logits, torch.tensor(values['labels']).type(torch.long))\n",
        "        logits = 0\n",
        "\n",
        "        print(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        train_batch_loss = loss.item()\n",
        "        train_last_loss = train_batch_loss / batch_size\n",
        "\n",
        "        iter_number += 1\n",
        "\n",
        "        print('Training batch {} last loss: {}'.format(i + 1, train_last_loss))\n",
        "    # Logging epoch-wise training loss\n",
        "    print(f\"\\nTraining epoch {epoch + 1} loss: \",train_last_loss)\n",
        "\n",
        "    bert_model.eval()\n",
        "    correct = 0\n",
        "    test_pred = []\n",
        "    for i, values in enumerate(test_dataset):\n",
        "\n",
        "        # We don't need gradients for testing\n",
        "        with torch.no_grad():\n",
        "            for multiplier in range(6):\n",
        "\n",
        "                val1 = torch.tensor(values['input_ids'][0][multiplier*512:(multiplier+1)*512])\n",
        "                val1 = val1.unsqueeze(dim=0)\n",
        "                val2 = torch.tensor(values['attention_mask'][0][multiplier*512:(multiplier+1)*512])\n",
        "                val2 = val2.unsqueeze(dim=0)\n",
        "\n",
        "                outputs = bert_model(val1, val2)\n",
        "                logits = logits + outputs.logits\n",
        "\n",
        "        logits = logits/6\n",
        "        loss = loss_fn(logits, torch.tensor(values['labels']).type(torch.long))\n",
        "\n",
        "        # Calculating total batch loss using the logits and labels\n",
        "        test_batch_loss = loss.item()\n",
        "\n",
        "        # Calculating the mean batch loss\n",
        "        test_last_loss = test_batch_loss / batch_size\n",
        "        print('Testing batch {} loss: {}'.format(i + 1, test_last_loss))\n",
        "\n",
        "        # correct += (logits.argmax(1) == values['labels']).sum().item()\n",
        "        print(logits.argmax(1), values['labels'])\n",
        "        # print(\"Testing accuracy: \",correct/((i + 1) * batch_size))\n",
        "        logits = 0\n",
        "\n",
        "\n",
        "    print(f\"\\nTesting epoch {epoch + 1} last loss: \",test_last_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0542559a-7cb6-47ae-b26f-060ccf2a6c40",
      "metadata": {
        "id": "0542559a-7cb6-47ae-b26f-060ccf2a6c40"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}