{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "1718f5a7-757c-4ba4-9336-ccca8690fb21",
      "metadata": {
        "id": "1718f5a7-757c-4ba4-9336-ccca8690fb21"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn import preprocessing\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import transformers as transform\n",
        "import torch\n",
        "from torch.nn.functional import softmax, normalize\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "aJlmoB8ycBf4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJlmoB8ycBf4",
        "outputId": "6f52d221-6a6b-4030-add6-34acb82cc80a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /MyDrive; to attempt to forcibly remount, call drive.mount(\"/MyDrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/MyDrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d2f194ce-bbb0-4038-8305-c86578cd3db8",
      "metadata": {
        "id": "d2f194ce-bbb0-4038-8305-c86578cd3db8"
      },
      "outputs": [],
      "source": [
        "class Bert_Regression:\n",
        "\n",
        "    model_class = transform.DistilBertForSequenceClassification\n",
        "\n",
        "    tokenizer_class = transform.DistilBertTokenizerFast\n",
        "\n",
        "    pretrained_weights = 'distilbert-base-uncased'\n",
        "    tokenizer = 0\n",
        "    model = 0\n",
        "\n",
        "    def initilize_tokenizer(self):\n",
        "        self.tokenizer = self.tokenizer_class.from_pretrained(self.pretrained_weights)\n",
        "\n",
        "        return self.tokenizer\n",
        "\n",
        "    def initilize_model(self):\n",
        "        self.model = self.model_class.from_pretrained(self.pretrained_weights, num_labels=10, max_position_embeddings=512)\n",
        "\n",
        "\n",
        "        return self.model\n",
        "\n",
        "    def clear_model_and_tokenizer(self):\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "\n",
        "    def get_reviewer_data(self, reviewer_names):\n",
        "\n",
        "        reviewer_ids = pd.DataFrame(columns=['ids'])\n",
        "        reviewer_ratings = pd.DataFrame(columns=['ratings'])\n",
        "\n",
        "        reviewer_file_ids = ['../MyDrive/MyDrive/scaledata/'+reviewer_name+'/id.'+reviewer_name for reviewer_name in reviewer_names]\n",
        "        reviewer_file_ratings = ['../MyDrive/MyDrive/scaledata/'+reviewer_name+'/rating.'+reviewer_name for reviewer_name in reviewer_names]\n",
        "        reviews_folder = ['../MyDrive/MyDrive/scale_whole_review/'+reviewer_name+'/txt.parag' for reviewer_name in reviewer_names]\n",
        "\n",
        "        count = 0\n",
        "\n",
        "        for reviewer_id in reviewer_file_ids:\n",
        "            print(reviewer_file_ids)\n",
        "            id_rating = pd.read_csv(reviewer_id, names = ['ids'])\n",
        "            id_rating['full_review'] = ''\n",
        "\n",
        "            for id in id_rating['ids']:\n",
        "                path = reviews_folder[count]+'/'+str(id)+'.txt'\n",
        "                f = open(path, 'r', encoding='cp1252')\n",
        "                text = f.read()\n",
        "                text = re.sub(r'[^\\w]', ' ', text)\n",
        "                text = re.sub(\"\\d+\", \"\", text)\n",
        "                text = text.lower()\n",
        "                id_rating.loc[id_rating['ids'] == id, 'full_review'] = text\n",
        "                # id_rating.loc[id == dennis_fulls, 'full_review'] = text\n",
        "                # print(id_rating)\n",
        "                f.close()\n",
        "                # print(path)\n",
        "\n",
        "            reviewer_ids = pd.concat([reviewer_ids, id_rating], axis=0)\n",
        "            count = count + 1\n",
        "\n",
        "        for reviewer_rating in reviewer_file_ratings:\n",
        "            # print(reviewer_id)\n",
        "            id_rating = pd.read_csv(reviewer_rating, names = ['ratings'])#.round()\n",
        "\n",
        "            reviewer_ratings = pd.concat([reviewer_ratings, id_rating], axis=0)\n",
        "            # print(reviewer_ratings)\n",
        "\n",
        "\n",
        "        reviewer_ids.reset_index(drop=True, inplace=True)\n",
        "        reviewer_ratings.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        reviewer_data = pd.concat([reviewer_ids, reviewer_ratings], axis=1)\n",
        "\n",
        "\n",
        "        return reviewer_data\n",
        "\n",
        "    def align_data(self, tokenized):\n",
        "        max_length = max([len(i) for i in tokenized])\n",
        "\n",
        "        features = pd.DataFrame(columns=range(max_length))\n",
        "\n",
        "        for i in range(len(tokenized)):\n",
        "            # features = features.append(pd.DataFrame(dennis_full['tokenized_sentences'][i]).transpose())\n",
        "            features = pd.concat([features, pd.DataFrame(tokenized[i]).transpose()])\n",
        "            # features = features.shift(-1)\n",
        "            # features.iloc[-1, :] = features.append(pd.DataFrame(dennis_full['tokenized_sentences'][i]).transpose())\n",
        "        features.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        return features\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ExtendedBertClassification(torch.nn.Module):\n",
        "    def __init__(self, BertModel):\n",
        "        super(ExtendedBertClassification, self).__init__()\n",
        "        self.bert_model = BertModel.initilize_model()\n",
        "        self.linear = torch.nn.Linear(10, 1)\n",
        "\n",
        "    def calculate(self, input_ids = None, attention_mask = None, max_review_lenght = None):\n",
        "        logits = 0\n",
        "        multiplier = 0\n",
        "\n",
        "        cntr = (max_review_lenght//512)+1\n",
        "\n",
        "        for multiplier in range(cntr):\n",
        "\n",
        "            if (cntr - multiplier) != 1:\n",
        "                val1 = torch.tensor(input_ids[0][multiplier*512:(multiplier+1)*512])\n",
        "                val1 = val1.clone().detach().unsqueeze(dim=0)\n",
        "                val2 = torch.tensor(attention_mask[0][multiplier*512:(multiplier+1)*512])\n",
        "                val2 = val2.clone().detach().unsqueeze(dim=0)\n",
        "            else:\n",
        "                val1 = torch.tensor(input_ids[0][multiplier*512:])\n",
        "                val1 = val1.clone().detach().unsqueeze(dim=0)\n",
        "                val2 = torch.tensor(attention_mask[0][multiplier*512:])\n",
        "                val2 = val2.clone().detach().unsqueeze(dim=0)\n",
        "\n",
        "            outputs = self.bert_model(val1, val2)\n",
        "\n",
        "            logits = logits + outputs.logits\n",
        "        logits = logits/6\n",
        "        return logits\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, max_review_lenght = None):\n",
        "        output = self.calculate(input_ids, attention_mask, max_review_lenght)\n",
        "        output = self.linear(output[0])\n",
        "        return output\n",
        "\n"
      ],
      "metadata": {
        "id": "MA2z7X4rpfM2"
      },
      "id": "MA2z7X4rpfM2",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "X7Z62M8eTXo3",
      "metadata": {
        "id": "X7Z62M8eTXo3"
      },
      "outputs": [],
      "source": [
        "class TokenData(Dataset):\n",
        "    def __init__(self, train = False):\n",
        "        if train:\n",
        "            self.text_data = reviewers_data['full_review']\n",
        "            self.tokens = train_tokens\n",
        "            self.labels = list(reviewers_data['ratings'] * 10)\n",
        "        else:\n",
        "            self.text_data = reviewers_test_data['full_review']\n",
        "            self.tokens = test_tokens\n",
        "            self.labels = list(reviewers_test_data['ratings'] * 10)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = {}\n",
        "        for k, v in self.tokens.items():\n",
        "            sample[k] = torch.tensor(v[idx])\n",
        "        sample['labels'] = torch.tensor(self.labels[idx])\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = Bert_Regression()"
      ],
      "metadata": {
        "id": "ekyoX8lM-CWm"
      },
      "id": "ekyoX8lM-CWm",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "01d360f7-4d13-445a-a5a6-a34163b47dc7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01d360f7-4d13-445a-a5a6-a34163b47dc7",
        "outputId": "94bcb291-cf3a-4461-f1e4-acafe9a05507"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['../MyDrive/MyDrive/scaledata/Scott+Renshaw/id.Scott+Renshaw']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "#Training dataset\n",
        "reviewers_data = x.get_reviewer_data(['Scott+Renshaw'])\n",
        "#James+Berardinelli Dennis+Schwartz Scott+Renshaw\n",
        "tokenizer = x.initilize_tokenizer()\n",
        "\n",
        "max_train_review_lenght = max(len(i.split()) for i in reviewers_data['full_review'])\n",
        "\n",
        "train_tokens = tokenizer(list(reviewers_data['full_review']), padding='max_length', truncation=False, max_length=max_train_review_lenght)\n",
        "train_tokens['labels'] = reviewers_data['ratings']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test dataset\n",
        "reviewers_test_data = x.get_reviewer_data(['James+Berardinelli'])\n",
        "#James+Berardinelli Dennis+Schwartz Scott+Renshaw\n",
        "\n",
        "max_test_review_lenght = max(len(i.split()) for i in reviewers_test_data['full_review'])\n",
        "\n",
        "test_tokens = tokenizer(list(reviewers_test_data['full_review']), padding='max_length', truncation=False, max_length=max_test_review_lenght)\n",
        "test_tokens['labels'] = reviewers_test_data['ratings']"
      ],
      "metadata": {
        "id": "SlRcFvbmlw74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2868275-5508-4f09-9760-38f217f1b583"
      },
      "id": "SlRcFvbmlw74",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['../MyDrive/MyDrive/scaledata/James+Berardinelli/id.James+Berardinelli']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (max_train_review_lenght//512)+1"
      ],
      "metadata": {
        "id": "wDTENDi-DOxd"
      },
      "id": "wDTENDi-DOxd",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "807bba9a-1871-4082-8cf4-eb1487ce7fa1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "807bba9a-1871-4082-8cf4-eb1487ce7fa1",
        "outputId": "17d7b9b6-5859-4a61-e183-207412fb4c4b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_mask', 'labels'])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "train_tokens.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "13447756-e5d7-44f0-9ede-40597faad64a",
      "metadata": {
        "id": "13447756-e5d7-44f0-9ede-40597faad64a"
      },
      "outputs": [],
      "source": [
        "#Train data\n",
        "batch_size = 1\n",
        "train_data = TokenData(train = True)\n",
        "train_dataset = DataLoader(train_data, shuffle=True, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test data\n",
        "batch_size = 1\n",
        "test_data = TokenData(train = False)\n",
        "test_dataset = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "Fh0_NuZVl2qn"
      },
      "id": "Fh0_NuZVl2qn",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b6600d4e-3900-4bb4-ba0b-cc451cc314fb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6600d4e-3900-4bb4-ba0b-cc451cc314fb",
        "outputId": "f6cfd695-2afb-42c3-f9ee-d1630b579551"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "extended_model = ExtendedBertClassification(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "b5a660b2-d257-4e45-8fce-aed4cadd0b4b",
      "metadata": {
        "id": "b5a660b2-d257-4e45-8fce-aed4cadd0b4b"
      },
      "outputs": [],
      "source": [
        "# optim func\n",
        "optimizer = torch.optim.AdamW(extended_model.parameters(), lr=0.01)\n",
        "# loss func\n",
        "loss_fn = torch.nn.MSELoss()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 3\n",
        "iter_number = 0\n",
        "iter_number2 = 0\n",
        "for epoch in range(epochs):\n",
        "    extended_model.train()\n",
        "    for i, values in enumerate(train_dataset):\n",
        "        #REMOVE IF BELOW IF NEEDED\n",
        "        if iter_number > 5:\n",
        "            break\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # print(len(values['input_ids'][0]), len(values['attention_mask'][0]))\n",
        "        outputs = extended_model(values['input_ids'], values['attention_mask'], max_train_review_lenght)\n",
        "\n",
        "        actual_y = torch.tensor(values['labels'])#.type(torch.long)\n",
        "        print(outputs, actual_y)\n",
        "        # break\n",
        "        loss = loss_fn(outputs, actual_y)\n",
        "\n",
        "        print('Loss: ', loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        train_batch_loss = loss.item()\n",
        "        train_last_loss = train_batch_loss / batch_size\n",
        "\n",
        "\n",
        "        print('Training batch {} last loss: {}'.format(i + 1, train_last_loss))\n",
        "        iter_number += 1\n",
        "\n",
        "    extended_model.eval()\n",
        "    for i, values in enumerate(test_dataset):\n",
        "        #REMOVE IF BELOW IF NEEDED\n",
        "        if iter_number2 > 20:\n",
        "            break\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = extended_model(values['input_ids'], values['attention_mask'], max_test_review_lenght)\n",
        "\n",
        "        actual_y = torch.tensor(values['labels'])#.type(torch.long)\n",
        "\n",
        "        print(outputs, actual_y)\n",
        "\n",
        "        loss = loss_fn(outputs, actual_y)\n",
        "\n",
        "        print('Loss: ', loss.item())\n",
        "\n",
        "        test_batch_loss = loss.item()\n",
        "        test_last_loss = test_batch_loss / batch_size\n",
        "\n",
        "\n",
        "        print('Test batch {} last loss: {}'.format(i + 1, test_last_loss))\n",
        "\n",
        "        iter_number2 += 1\n"
      ],
      "metadata": {
        "id": "2-OSFpVDBdHN"
      },
      "id": "2-OSFpVDBdHN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LboH5ywxZC2l"
      },
      "id": "LboH5ywxZC2l",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}