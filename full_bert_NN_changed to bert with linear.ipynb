{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "1718f5a7-757c-4ba4-9336-ccca8690fb21",
      "metadata": {
        "id": "1718f5a7-757c-4ba4-9336-ccca8690fb21"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn import preprocessing\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import transformers as transform\n",
        "import torch\n",
        "from torch.nn.functional import softmax, normalize\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "aJlmoB8ycBf4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJlmoB8ycBf4",
        "outputId": "013e3f4f-99a6-42f9-e28f-f86f2f0f8f86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /MyDrive; to attempt to forcibly remount, call drive.mount(\"/MyDrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/MyDrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "d2f194ce-bbb0-4038-8305-c86578cd3db8",
      "metadata": {
        "id": "d2f194ce-bbb0-4038-8305-c86578cd3db8"
      },
      "outputs": [],
      "source": [
        "class Bert_Regression:\n",
        "\n",
        "    model_class = transform.DistilBertForSequenceClassification\n",
        "\n",
        "    tokenizer_class = transform.DistilBertTokenizerFast\n",
        "\n",
        "    pretrained_weights = 'distilbert-base-uncased'\n",
        "    tokenizer = 0\n",
        "    model = 0\n",
        "\n",
        "    def initilize_tokenizer(self):\n",
        "        self.tokenizer = self.tokenizer_class.from_pretrained(self.pretrained_weights)\n",
        "\n",
        "        return self.tokenizer\n",
        "\n",
        "    def initilize_model(self):\n",
        "        self.model = self.model_class.from_pretrained(self.pretrained_weights, num_labels=10, max_position_embeddings=512)\n",
        "\n",
        "\n",
        "        return self.model\n",
        "\n",
        "    def clear_model_and_tokenizer(self):\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "\n",
        "    def get_reviewer_data(self, reviewer_names):\n",
        "\n",
        "        reviewer_ids = pd.DataFrame(columns=['ids'])\n",
        "        reviewer_ratings = pd.DataFrame(columns=['ratings'])\n",
        "\n",
        "        reviewer_file_ids = ['../MyDrive/MyDrive/scaledata/'+reviewer_name+'/id.'+reviewer_name for reviewer_name in reviewer_names]\n",
        "        reviewer_file_ratings = ['../MyDrive/MyDrive/scaledata/'+reviewer_name+'/rating.'+reviewer_name for reviewer_name in reviewer_names]\n",
        "        reviews_folder = ['../MyDrive/MyDrive/scale_whole_review/'+reviewer_name+'/txt.parag' for reviewer_name in reviewer_names]\n",
        "\n",
        "        count = 0\n",
        "\n",
        "        for reviewer_id in reviewer_file_ids:\n",
        "            print(reviewer_file_ids)\n",
        "            id_rating = pd.read_csv(reviewer_id, names = ['ids'])\n",
        "            id_rating['full_review'] = ''\n",
        "\n",
        "            for id in id_rating['ids']:\n",
        "                path = reviews_folder[count]+'/'+str(id)+'.txt'\n",
        "                f = open(path, 'r', encoding='cp1252')\n",
        "                text = f.read()\n",
        "                text = re.sub(r'[^\\w]', ' ', text)\n",
        "                text = re.sub(\"\\d+\", \"\", text)\n",
        "                text = text.lower()\n",
        "                id_rating.loc[id_rating['ids'] == id, 'full_review'] = text\n",
        "                # id_rating.loc[id == dennis_fulls, 'full_review'] = text\n",
        "                # print(id_rating)\n",
        "                f.close()\n",
        "                # print(path)\n",
        "\n",
        "            reviewer_ids = pd.concat([reviewer_ids, id_rating], axis=0)\n",
        "            count = count + 1\n",
        "\n",
        "        for reviewer_rating in reviewer_file_ratings:\n",
        "            # print(reviewer_id)\n",
        "            id_rating = pd.read_csv(reviewer_rating, names = ['ratings'])#.round()\n",
        "\n",
        "            reviewer_ratings = pd.concat([reviewer_ratings, id_rating], axis=0)\n",
        "            # print(reviewer_ratings)\n",
        "\n",
        "\n",
        "        reviewer_ids.reset_index(drop=True, inplace=True)\n",
        "        reviewer_ratings.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        reviewer_data = pd.concat([reviewer_ids, reviewer_ratings], axis=1)\n",
        "\n",
        "\n",
        "        return reviewer_data\n",
        "\n",
        "    def align_data(self, tokenized):\n",
        "        max_length = max([len(i) for i in tokenized])\n",
        "\n",
        "        features = pd.DataFrame(columns=range(max_length))\n",
        "\n",
        "        for i in range(len(tokenized)):\n",
        "            # features = features.append(pd.DataFrame(dennis_full['tokenized_sentences'][i]).transpose())\n",
        "            features = pd.concat([features, pd.DataFrame(tokenized[i]).transpose()])\n",
        "            # features = features.shift(-1)\n",
        "            # features.iloc[-1, :] = features.append(pd.DataFrame(dennis_full['tokenized_sentences'][i]).transpose())\n",
        "        features.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        return features\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ExtendedBertClassification(torch.nn.Module):\n",
        "    def __init__(self, BertModel):\n",
        "        super(ExtendedBertClassification, self).__init__()\n",
        "        self.bert_model = BertModel.initilize_model()\n",
        "        self.linear = torch.nn.Linear(10, 1)\n",
        "\n",
        "    def calculate(self, input_ids = None, attention_mask = None):\n",
        "        logits = 0\n",
        "        multiplier = 0\n",
        "        for multiplier in range(7):\n",
        "\n",
        "            val1 = torch.tensor(input_ids[0][multiplier*512:(multiplier+1)*512])\n",
        "            val1 = val1.clone().detach().unsqueeze(dim=0)\n",
        "            val2 = torch.tensor(attention_mask[0][multiplier*512:(multiplier+1)*512])\n",
        "            val2 = val2.clone().detach().unsqueeze(dim=0)\n",
        "\n",
        "            outputs = self.bert_model(val1, val2)\n",
        "\n",
        "            logits = logits + outputs.logits\n",
        "        logits = logits/6\n",
        "        return logits\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output = self.calculate(input_ids, attention_mask)\n",
        "        output = self.linear(output[0])\n",
        "        return output\n",
        "\n"
      ],
      "metadata": {
        "id": "MA2z7X4rpfM2"
      },
      "id": "MA2z7X4rpfM2",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "X7Z62M8eTXo3",
      "metadata": {
        "id": "X7Z62M8eTXo3"
      },
      "outputs": [],
      "source": [
        "class TokenData(Dataset):\n",
        "    def __init__(self, train = False):\n",
        "        if train:\n",
        "            self.text_data = reviewers_data['full_review']\n",
        "            self.tokens = train_tokens\n",
        "            self.labels = list(reviewers_data['ratings'] * 10)\n",
        "        else:\n",
        "            self.text_data = reviewers_test_data['full_review']\n",
        "            self.tokens = test_tokens\n",
        "            self.labels = list(reviewers_test_data['ratings'] * 10)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = {}\n",
        "        for k, v in self.tokens.items():\n",
        "            sample[k] = torch.tensor(v[idx])\n",
        "        sample['labels'] = torch.tensor(self.labels[idx])\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = Bert_Regression()"
      ],
      "metadata": {
        "id": "ekyoX8lM-CWm"
      },
      "id": "ekyoX8lM-CWm",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "01d360f7-4d13-445a-a5a6-a34163b47dc7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01d360f7-4d13-445a-a5a6-a34163b47dc7",
        "outputId": "cf361ba1-4eea-4afb-ddfa-a08cad7ea244"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['../MyDrive/MyDrive/scaledata/Dennis+Schwartz/id.Dennis+Schwartz']\n"
          ]
        }
      ],
      "source": [
        "#Training dataset\n",
        "reviewers_data = x.get_reviewer_data(['Dennis+Schwartz'])\n",
        "#James+Berardinelli Dennis+Schwartz Scott+Renshaw\n",
        "tokenizer = x.initilize_tokenizer()\n",
        "\n",
        "train_tokens = tokenizer(list(reviewers_data['full_review']), padding='max_length', truncation=False, max_length=3210)\n",
        "train_tokens['labels'] = reviewers_data['ratings']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test dataset\n",
        "reviewers_test_data = x.get_reviewer_data(['Scott+Renshaw'])\n",
        "#James+Berardinelli Dennis+Schwartz Scott+Renshaw\n",
        "\n",
        "test_tokens = tokenizer(list(reviewers_test_data['full_review']), padding='max_length', truncation=False, max_length=3210)\n",
        "test_tokens['labels'] = reviewers_test_data['ratings']"
      ],
      "metadata": {
        "id": "SlRcFvbmlw74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec8760b2-aa9f-4527-81ef-c4dff316353e"
      },
      "id": "SlRcFvbmlw74",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['../MyDrive/MyDrive/scaledata/Scott+Renshaw/id.Scott+Renshaw']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "807bba9a-1871-4082-8cf4-eb1487ce7fa1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "807bba9a-1871-4082-8cf4-eb1487ce7fa1",
        "outputId": "8f4187c6-7f59-4603-c152-45331048ed72"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_mask', 'labels'])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "train_tokens.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "13447756-e5d7-44f0-9ede-40597faad64a",
      "metadata": {
        "id": "13447756-e5d7-44f0-9ede-40597faad64a"
      },
      "outputs": [],
      "source": [
        "#Train data\n",
        "batch_size = 1\n",
        "train_data = TokenData(train = True)\n",
        "train_dataset = DataLoader(train_data, shuffle=True, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test data\n",
        "batch_size = 1\n",
        "test_data = TokenData(train = False)\n",
        "test_dataset = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "Fh0_NuZVl2qn"
      },
      "id": "Fh0_NuZVl2qn",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "b6600d4e-3900-4bb4-ba0b-cc451cc314fb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6600d4e-3900-4bb4-ba0b-cc451cc314fb",
        "outputId": "3cf8cb75-8def-4711-df4c-57fc556a178f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "extended_model = ExtendedBertClassification(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "b5a660b2-d257-4e45-8fce-aed4cadd0b4b",
      "metadata": {
        "id": "b5a660b2-d257-4e45-8fce-aed4cadd0b4b"
      },
      "outputs": [],
      "source": [
        "# optim func\n",
        "optimizer = torch.optim.AdamW(extended_model.parameters(), lr=0.01)\n",
        "# loss func\n",
        "loss_fn = torch.nn.MSELoss()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 3\n",
        "iter_number = 0\n",
        "iter_number2 = 0\n",
        "for epoch in range(epochs):\n",
        "    extended_model.train()\n",
        "    for i, values in enumerate(train_dataset):\n",
        "        #REMOVE IF BELOW IF NEEDED\n",
        "        if iter_number > 20:\n",
        "            break\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = extended_model(values['input_ids'], values['attention_mask'])\n",
        "\n",
        "        actual_y = torch.tensor(values['labels'])#.type(torch.long)\n",
        "        print(outputs, actual_y)\n",
        "        # break\n",
        "        loss = loss_fn(outputs, actual_y)\n",
        "\n",
        "        print(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        train_batch_loss = loss.item()\n",
        "        train_last_loss = train_batch_loss / batch_size\n",
        "\n",
        "\n",
        "        print('Training batch {} last loss: {}'.format(i + 1, train_last_loss))\n",
        "        iter_number += 1\n",
        "\n",
        "    extended_model.eval()\n",
        "    for i, values in enumerate(test_dataset):\n",
        "        #REMOVE IF BELOW IF NEEDED\n",
        "        if iter_number2 > 20:\n",
        "            break\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = extended_model(values['input_ids'], values['attention_mask'])\n",
        "\n",
        "        actual_y = torch.tensor(values['labels'])#.type(torch.long)\n",
        "\n",
        "        print(outputs, actual_y)\n",
        "\n",
        "        loss = loss_fn(outputs, actual_y)\n",
        "\n",
        "        print(loss.item())\n",
        "\n",
        "        test_batch_loss = loss.item()\n",
        "        test_last_loss = test_batch_loss / batch_size\n",
        "\n",
        "\n",
        "        print('Test batch {} last loss: {}'.format(i + 1, test_last_loss))\n",
        "\n",
        "        iter_number2 += 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-OSFpVDBdHN",
        "outputId": "6dc919d2-c083-466a-f7e8-da3e2611adc2"
      },
      "id": "2-OSFpVDBdHN",
      "execution_count": 31,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-19-7457fe9fa7e0>:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val1 = torch.tensor(input_ids[0][multiplier*512:(multiplier+1)*512])\n",
            "<ipython-input-19-7457fe9fa7e0>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val2 = torch.tensor(attention_mask[0][multiplier*512:(multiplier+1)*512])\n",
            "<ipython-input-31-23bf661c3540>:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  actual_y = torch.tensor(values['labels'])#.type(torch.long)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1678], grad_fn=<ViewBackward0>) tensor([4.])\n",
            "14.685408592224121\n",
            "Training batch 1 last loss: 14.685408592224121\n",
            "tensor([1.4817], grad_fn=<ViewBackward0>) tensor([3.])\n",
            "2.3052480220794678\n",
            "Training batch 2 last loss: 2.3052480220794678\n",
            "tensor([49.9564], grad_fn=<ViewBackward0>) tensor([4.])\n",
            "2111.989013671875\n",
            "Training batch 3 last loss: 2111.989013671875\n",
            "tensor([5.8807], grad_fn=<ViewBackward0>) tensor([6.])\n",
            "0.01423769723623991\n",
            "Training batch 4 last loss: 0.01423769723623991\n",
            "tensor([16.8523], grad_fn=<ViewBackward0>) tensor([9.])\n",
            "61.658447265625\n",
            "Training batch 5 last loss: 61.658447265625\n",
            "tensor([-0.7422], grad_fn=<ViewBackward0>) tensor([3.])\n",
            "14.003778457641602\n",
            "Training batch 6 last loss: 14.003778457641602\n",
            "tensor([-0.2009], grad_fn=<ViewBackward0>) tensor([4.])\n",
            "17.64727783203125\n",
            "Training batch 7 last loss: 17.64727783203125\n",
            "tensor([0.5707], grad_fn=<ViewBackward0>) tensor([7.])\n",
            "41.33552932739258\n",
            "Training batch 8 last loss: 41.33552932739258\n",
            "tensor([0.7281], grad_fn=<ViewBackward0>) tensor([5.])\n",
            "18.249216079711914\n",
            "Training batch 9 last loss: 18.249216079711914\n",
            "tensor([1.5471], grad_fn=<ViewBackward0>) tensor([8.])\n",
            "41.64039611816406\n",
            "Training batch 10 last loss: 41.64039611816406\n",
            "tensor([2.4637], grad_fn=<ViewBackward0>) tensor([3.])\n",
            "0.2876557409763336\n",
            "Training batch 11 last loss: 0.2876557409763336\n",
            "tensor([3.5533], grad_fn=<ViewBackward0>) tensor([5.])\n",
            "2.09295916557312\n",
            "Training batch 12 last loss: 2.09295916557312\n",
            "tensor([4.0292], grad_fn=<ViewBackward0>) tensor([5.])\n",
            "0.9424441456794739\n",
            "Training batch 13 last loss: 0.9424441456794739\n",
            "tensor([6.4376], grad_fn=<ViewBackward0>) tensor([7.])\n",
            "0.31632739305496216\n",
            "Training batch 14 last loss: 0.31632739305496216\n",
            "tensor([7.8947], grad_fn=<ViewBackward0>) tensor([4.])\n",
            "15.168725967407227\n",
            "Training batch 15 last loss: 15.168725967407227\n",
            "tensor([6.4313], grad_fn=<ViewBackward0>) tensor([6.])\n",
            "0.18599967658519745\n",
            "Training batch 16 last loss: 0.18599967658519745\n",
            "tensor([5.6888], grad_fn=<ViewBackward0>) tensor([6.])\n",
            "0.0968235656619072\n",
            "Training batch 17 last loss: 0.0968235656619072\n",
            "tensor([7.5867], grad_fn=<ViewBackward0>) tensor([7.])\n",
            "0.3442263603210449\n",
            "Training batch 18 last loss: 0.3442263603210449\n",
            "tensor([7.6118], grad_fn=<ViewBackward0>) tensor([5.])\n",
            "6.821492671966553\n",
            "Training batch 19 last loss: 6.821492671966553\n",
            "tensor([5.7761], grad_fn=<ViewBackward0>) tensor([7.])\n",
            "1.4979355335235596\n",
            "Training batch 20 last loss: 1.4979355335235596\n",
            "tensor([6.5336], grad_fn=<ViewBackward0>) tensor([3.])\n",
            "12.486634254455566\n",
            "Training batch 21 last loss: 12.486634254455566\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-23bf661c3540>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  actual_y = torch.tensor(values['labels'])#.type(torch.long)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([5.4102]) tensor([7.])\n",
            "2.527327299118042\n",
            "Test batch 1 last loss: 12.486634254455566\n",
            "tensor([5.4102]) tensor([4.])\n",
            "1.9887853860855103\n",
            "Test batch 2 last loss: 12.486634254455566\n",
            "tensor([5.4102]) tensor([5.])\n",
            "0.1682993471622467\n",
            "Test batch 3 last loss: 12.486634254455566\n",
            "tensor([5.4102]) tensor([3.])\n",
            "5.809271335601807\n",
            "Test batch 4 last loss: 12.486634254455566\n",
            "tensor([5.4102]) tensor([6.])\n",
            "0.34781327843666077\n",
            "Test batch 5 last loss: 12.486634254455566\n",
            "tensor([5.4102]) tensor([2.])\n",
            "11.62975788116455\n",
            "Test batch 6 last loss: 12.486634254455566\n",
            "tensor([5.4102]) tensor([3.])\n",
            "5.809271335601807\n",
            "Test batch 7 last loss: 12.486634254455566\n",
            "tensor([5.4102]) tensor([6.])\n",
            "0.34781327843666077\n",
            "Test batch 8 last loss: 12.486634254455566\n",
            "tensor([5.4102]) tensor([6.])\n",
            "0.34781327843666077\n",
            "Test batch 9 last loss: 12.486634254455566\n",
            "tensor([5.4102]) tensor([8.])\n",
            "6.706840991973877\n",
            "Test batch 10 last loss: 12.486634254455566\n",
            "tensor([5.4102]) tensor([5.])\n",
            "0.1682993471622467\n",
            "Test batch 11 last loss: 12.486634254455566\n",
            "tensor([5.4102]) tensor([6.])\n",
            "0.34781327843666077\n",
            "Test batch 12 last loss: 12.486634254455566\n",
            "tensor([5.4102]) tensor([7.])\n",
            "2.527327299118042\n",
            "Test batch 13 last loss: 12.486634254455566\n",
            "tensor([5.4102]) tensor([4.])\n",
            "1.9887853860855103\n",
            "Test batch 14 last loss: 12.486634254455566\n",
            "tensor([5.4102]) tensor([5.])\n",
            "0.1682993471622467\n",
            "Test batch 15 last loss: 12.486634254455566\n",
            "tensor([5.4102]) tensor([5.])\n",
            "0.1682993471622467\n",
            "Test batch 16 last loss: 12.486634254455566\n",
            "tensor([5.4102]) tensor([6.])\n",
            "0.34781327843666077\n",
            "Test batch 17 last loss: 12.486634254455566\n",
            "tensor([5.4102]) tensor([6.])\n",
            "0.34781327843666077\n",
            "Test batch 18 last loss: 12.486634254455566\n",
            "tensor([5.4102]) tensor([7.])\n",
            "2.527327299118042\n",
            "Test batch 19 last loss: 12.486634254455566\n",
            "tensor([5.4102]) tensor([2.])\n",
            "11.62975788116455\n",
            "Test batch 20 last loss: 12.486634254455566\n",
            "tensor([5.4102]) tensor([4.])\n",
            "1.9887853860855103\n",
            "Test batch 21 last loss: 12.486634254455566\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n9mQ0Ex0n4A8"
      },
      "id": "n9mQ0Ex0n4A8",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}